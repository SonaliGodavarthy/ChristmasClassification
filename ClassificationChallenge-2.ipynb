{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b7cd2d2-b94c-4238-8b51-759bd4d398b1",
   "metadata": {},
   "source": [
    "# Classification Challenge Mark 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc7abd4-bbf4-470d-b2dc-fb34432ac20b",
   "metadata": {},
   "source": [
    "## Given Template"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51f9aaf-0505-445a-acf7-2c296c75fb1e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e17af7-8d0f-497e-be1e-a0f7ff8a9006",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class ChristmasImages(Dataset):\n",
    "    \n",
    "    def __init__(self, path, training=True):\n",
    "        super().__init__()\n",
    "        self.training = training\n",
    "        # If training == True, path contains subfolders\n",
    "        # containing images of the corresponding classes\n",
    "        # If training == False, path directly contains\n",
    "        # the test images for testing the classifier\n",
    "        pass\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # If self.training == False, output (image, )\n",
    "        # where image will be used as input for your model\n",
    "        raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4156ff-65d0-4744-91ac-9fd4cd3b2c22",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479e4afd-21c0-450c-b3d8-cd7b0e4da6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class Network(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        #############################\n",
    "        # Initialize your network\n",
    "        #############################\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        #############################\n",
    "        # Implement the forward pass\n",
    "        #############################\n",
    "        \n",
    "        pass\n",
    "    \n",
    "    def save_model(self):\n",
    "        \n",
    "        #############################\n",
    "        # Saving the model's weitghts\n",
    "        # Upload 'model' as part of\n",
    "        # your submission\n",
    "        # Do not modify this function\n",
    "        #############################\n",
    "        \n",
    "        torch.save(self.state_dict(), 'model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f887e257-8438-4cc1-9a81-811864ad3388",
   "metadata": {},
   "source": [
    "## CODE STARTS HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8752ad5-d066-4ec7-9db5-ddbc2b3f1c59",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from torchvision import models\n",
    "from functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ead7f93-11db-4e6f-9068-d1bf550bf519",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Train dataset path\n",
    "train_path = \"/mnt/datasets/deep_learning_challenge/train/\"\n",
    "# Val dataset path\n",
    "val_path = \"/mnt/datasets/deep_learning_challenge/val/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "832201ea-9003-4d66-81b9-66d5ecd55ca8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c424abc8-a158-46c7-b7f2-a8a81fc69db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChristmasImages(Dataset) :\n",
    "\n",
    "    def __init__(self, path, training=True):\n",
    "        super().__init__()\n",
    "        self.training = training\n",
    "        # If training == True, path contains subfolders\n",
    "        # containing images of the corresponding classes\n",
    "        # If training == False, path directly contains\n",
    "        # the test images for testing the classifier\n",
    "        \n",
    "        # The path to the dataset\n",
    "        self.path = path\n",
    "        \n",
    "        # The transformations to be applied to the images\n",
    "        self.transform = transforms.Compose([transforms.Resize((224,224)),\n",
    "                                             transforms.RandomHorizontalFlip(p=0.5),\n",
    "                                             transforms.RandomVerticalFlip(p=0.5),\n",
    "                                             transforms.ToTensor()])\n",
    "        \n",
    "        if(self.training):\n",
    "            # Creating an ImageFolder dataset\n",
    "            self.data = datasets.ImageFolder(self.path,transform=self.transform)\n",
    "        \n",
    "        else:\n",
    "            # Directly loading the images from the path\n",
    "            # Getting the paths of all images in the test directory\n",
    "            image_paths = [self.path+'/'+ i for i in os.listdir(self.path)]\n",
    "            # Applying transformations to each test image and stack them into a tensor\n",
    "            image_list = [self.transform(Image.open(image).convert('RGB')) for image in image_paths]\n",
    "            self.data = torch.stack(image_list,dim=0)\n",
    "                \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # If self.training == False, output (image, )\n",
    "        # where image will be used as input for your model\n",
    "        \n",
    "        if(self.training):\n",
    "            return (self.data[index][0], self.data[index][1]) # Returning the image and its corresponding label\n",
    "        else:\n",
    "            return (self.data[index],) # Returning only the transformed image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050eafee-3dc9-4a8c-a5fd-7dedef809fc1",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Googlenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2acac933-4c37-47d2-9855-9c0fa7053af9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class NetworkGooglenet(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        #############################\n",
    "        # Initialize your network\n",
    "        #############################\n",
    "        \n",
    "        # Loading the pre-trained GoogLeNet model with default weights\n",
    "        self.base_model = models.googlenet(weights=models.GoogLeNet_Weights.DEFAULT)\n",
    "        \n",
    "        # Extracting the feature extraction layers from the GoogLeNet model\n",
    "        self.features = nn.ModuleList(self.base_model.children())[:-1]\n",
    "        self.features = nn.Sequential(*self.features)\n",
    "        \n",
    "        # Freezing the parameters of the base model to prevent them from being updated during training\n",
    "        for param in self.base_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # Getting the number of input features for the fully connected layer\n",
    "        fc_inputs = self.base_model.fc.in_features\n",
    "        \n",
    "        # Defining the architecture of the fully connected layers\n",
    "        self.flat = nn.Flatten()\n",
    "        self.linear1 = nn.Linear(fc_inputs, 256)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(256,8)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        #############################\n",
    "        # Implement the forward pass\n",
    "        #############################\n",
    "        x = self.features(x)\n",
    "        x = self.flat(x)\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "    \n",
    "    def save_model(self):\n",
    "        \n",
    "        #############################\n",
    "        # Saving the model's weitghts\n",
    "        # Upload 'model' as part of\n",
    "        # your submission\n",
    "        # Do not modify this function\n",
    "        #############################\n",
    "        \n",
    "        torch.save(self.state_dict(), 'model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b664595-9e40-409f-8124-126cd48f8e8c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gnet = NetworkGooglenet()\n",
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d51f1ddf-5aec-486b-a629-5cf068232df0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gnet.load_state_dict(torch.load(\"gnet_epoch_35.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5aa7cae2-633f-4094-93e4-1768b5684a89",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data = ChristmasImages(train_path,True)\n",
    "# val_data = ChristmasImages(val_path,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "72f361aa-652f-4821-9b2c-f3670fb1f71b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 342\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2324474a-fc1f-4e7d-b324-38a7e95c3e1a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(gnet.parameters(),lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "111295af-2708-459a-80ee-e8f6b7f7eacb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_data,batch_size,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853eb01e-3cfc-4f83-ac41-0c6a179e1a18",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46cf32406b4c423ab0a65c16d805bc0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | train_loss: 1.4023 | train_acc: 0.8755 | \n",
      "Epoch: 2 | train_loss: 1.4025 | train_acc: 0.8753 | \n",
      "Epoch: 3 | train_loss: 1.3957 | train_acc: 0.8814 | \n",
      "Epoch: 4 | train_loss: 1.3973 | train_acc: 0.8800 | \n",
      "Epoch: 5 | train_loss: 1.3972 | train_acc: 0.8817 | \n",
      "Epoch: 6 | train_loss: 1.4038 | train_acc: 0.8725 | \n"
     ]
    }
   ],
   "source": [
    "gnet_res = training_data(gnet,train_dataloader,epochs,loss_fn,optimizer,device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02b55da-f286-497d-9b40-c15508aebb93",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_loss_curves(gnet_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb50bd4c-1182-44b3-be71-37f3cbe1e0e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gnet.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e00794-54cd-4f1a-b71a-2d9f07ff7459",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a10fd66-1e04-46f1-9f71-1be12bba192a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c6f4c7-749e-453c-9376-af6b7e991961",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ece5fb-6c24-4cff-9e6c-47ef4b0d0876",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "66d62754-c0d5-4249-8054-292ef0ccec58",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Inception V3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5eb8b4-e6b8-4a02-95db-6002c325b282",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data = ChristmasImages(train_path,True)\n",
    "val_data = ChristmasImages(val_path,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637abcb2-658c-41e4-bf90-c968551f704f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetworkInceptionV3(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        #############################\n",
    "        # Initialize your network\n",
    "        #############################\n",
    "        self.base_model = models.inception_v3(weights=models.Inception_V3_Weights.DEFAULT)\n",
    "        self.features = nn.ModuleList(self.base_model.children())[:-1]\n",
    "        self.features = nn.Sequential(*self.features)\n",
    "        for param in self.base_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        fc_inputs = self.base_model.fc.in_features\n",
    "        self.flat = nn.Flatten()\n",
    "        self.linear1 = nn.Linear(512, 128)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(128,8)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        #############################\n",
    "        # Implement the forward pass\n",
    "        #############################\n",
    "        x = self.features(x)\n",
    "        x = self.flat(x)\n",
    "        # x = self.relu1(self.linear1(x))\n",
    "        x = self.softmax(self.linear2(x))\n",
    "        return x\n",
    "    \n",
    "    def save_model(self):\n",
    "        \n",
    "        #############################\n",
    "        # Saving the model's weitghts\n",
    "        # Upload 'model' as part of\n",
    "        # your submission\n",
    "        # Do not modify this function\n",
    "        #############################\n",
    "        \n",
    "        torch.save(self.state_dict(), 'model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46eb7774-81c3-458d-8c75-4cf50b8550e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "incept_net = NetworkInceptionV3()\n",
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe753ea-7efd-4e09-8eed-e1699917f343",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "epochs = 15\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(incept_net.parameters(),lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf9a486-b82d-4356-85b8-829bc7fd9ba9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_data,batch_size,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e88a0b8-224d-4b43-981d-2fc68f6c7b64",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "incept_net_res = training_data(incept_net,train_dataloader,epochs,loss_fn,optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad63cae8-6068-4f76-b9b9-b90c27463755",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For inception padding error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ebdd28-221d-49d8-92e3-628f5c6f5bd3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## VGG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c56960-51c2-4046-b4a4-d55574f8fb63",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data = ChristmasImages(train_path,True)\n",
    "val_data = ChristmasImages(val_path,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53b66bd-d110-4215-b193-90700785520e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class NetworkVGG(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        #############################\n",
    "        # Initialize your network\n",
    "        #############################\n",
    "        self.base_model = models.vgg16(weights=models.VGG16_Weights.DEFAULT)\n",
    "\n",
    "        for param in self.base_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        # fc_inputs = self.base_model.fc.in_features\n",
    "        self.base_model.classifier[-1] = nn.Linear(4096,128)\n",
    "        # self.linear1 = nn.Linear(, 256)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(128,8)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        #############################\n",
    "        # Implement the forward pass\n",
    "        #############################\n",
    "        x = self.base_model(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "    def save_model(self):\n",
    "\n",
    "        #############################\n",
    "        # Saving the model's weitghts\n",
    "        # Upload 'model' as part of\n",
    "        # your submission\n",
    "        # Do not modify this function\n",
    "        #############################\n",
    "\n",
    "        torch.save(self.state_dict(), 'model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b35a012-def7-4055-866f-6200b37121da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vgg_model = NetworkVGG()\n",
    "device = \"cpu\"\n",
    "vgg_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b54936-e4e8-4566-9f72-ae3784751c23",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_data,batch_size,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a977544-184c-40e9-9957-2859f3f56c2a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ffd506a-5229-467d-a1a6-43685337c00b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(vgg_model.parameters(),lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adfa360a-0b0b-411c-b1f2-7585403f2092",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second reun for 15 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a054ebc-1301-4048-9e45-7c463dc0011b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vgg_model.load_state_dict(torch.load(\"vgg_epoch_31.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b74397-0667-45ff-b0f5-b5459b38d332",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vgg_res = training_data(vgg_model,train_dataloader,epochs,loss_fn,optimizer,device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927f784a-704f-49fc-b278-2c0821825f86",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vgg_model.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36452813-f9a0-4a0e-b91d-bf3095374d95",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
